""" Filename:     RL.py
    Author(s):    Thomas Bellucci
    Description:  Implementation of Upper Confidence Bounds (UCB) used to
                  select Thoughts generated by the RLChatbot to verbalize.
    Date created: Nov. 11th, 2021
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import warnings
import time
        

class UCB1:
    def __init__(self, c=10, decay=0.1, floor=0.1):
        self.__c = c
        self.__decay = decay
        self.__floor = floor
        self.__t = 0
        self.__Q = dict()

    @property
    def Q_table(self):
        """ Q table with action-value estimates """
        return {a:np.mean(q) for a,q in self.__Q.items()}

    def select_action(self, actions):
        """ Selects an action from the set of available actions that maximizes
            the average observed reward, taking into account uncertainty.

            params
            list actions: List of actions from which to select

            returns: an action
        """
        self.__t += 1

        # Score actions according to their expected value and uncertainty
        action_scores = []
        for action in actions:

            # Update action-value table if action has not been chosen before
            if action not in self.__Q:
                self.__Q[action] = []

            # Compute average reward + uncertainty
            if self.__Q[action] == []:
                score = np.inf # ensures all actions are sampled at least once
            else:
                score = np.mean(self.__Q[action]) + self.__c * np.sqrt(np.log(self.__t) / len(self.__Q[action]))
            action_scores.append((action, score))

        # Greedy selection
        selected_action, _ = max(action_scores, key=lambda x: x[1])
        return selected_action

    def update_utility(self, action, reward):
        """ Updates the action-value table (Q) by incrementally updating the average
            reward estimate of some action with the observed reward.

            params
            str action:    the selected action 
            float reward:  the reward obtained after performing the action

            returns: None
        """
        # Add reward to tally for action
        self.__Q[action].append(reward)

        # Decrease exploration until minimum is reached
        self.__c = max(self.__c - self.__decay, self.__floor)

    def plot(self, margin=0.05):
        """ Visualizes the Q scores and their uncertainties in a box plot.

            params
            float margin: Margin between bars

            returns: None
        """
        if len(self.__Q) == 0:
            warnings.warn("\tEmpty utility table")
            return
        
        # Estimate value/uncertainty
        Q, U = [], []
        actions = sorted(list(self.__Q.keys()))
        for action in actions:

            if self.__Q[action] == []:
                q = -0.1
            else:
                q = np.mean(self.__Q[action])

            u = self.__c * np.sqrt(np.log(self.__t) / (len(self.__Q[action]) + 1))

            Q.append(q)
            U.append(u)


        # Draw Q-value barplot
        fig = plt.figure(figsize=(10, 5), tight_layout=True)
        plt.suptitle("$t=${}, $c=${}".format(self.__t, self.__c))
        
        plt.subplot(1, 2, 1)
        plt.ylabel("$Uncertainty$ $(U)$")
        plt.xlabel("$Actions$ $(a)$")
        plt.xticks(np.arange(len(actions)), actions)
        plt.bar(np.arange(len(actions)), U)
        
        plt.subplot(1, 2, 2)
        plt.ylabel("$Utility$ $(Q)$")
        plt.xlabel("$Actions$ $(a)$")
        plt.xticks(np.arange(len(actions)), actions)
        plt.bar(np.arange(len(actions)), Q)
        
        plt.show()



if __name__ == "__main__":
    # Toy game with increasing reward probabilities.
    num_actions = 10
    actions = np.arange(num_actions)
    prob_reward = np.linspace(0.0, 1.0, num_actions)
    N = 100

    ucb = UCB1()
    for k in range(N):
        # What actions are available
        n = np.random.randint(3, num_actions)
        idx = np.random.choice(actions, size=n, replace=False)

        # Select one action from sample
        sampled_actions = actions[idx]
        action = ucb.select_action(sampled_actions)
        print(sampled_actions, action)

        # Draw reward from binomial distribution
        reward = np.random.binomial(1, prob_reward[action])

        # Update action reward
        ucb.update_utility(action, reward)

        if k > N * 0.8:
            ucb.plot()
"""
"""

""" Filename:     RL.py
    Author(s):    Thomas Bellucci
    Description:  Implementation of Upper Confidence Bounds (UCB) used to
                  select Thoughts generated by the RLChatbot to verbalize.
    Date created: Nov. 11th, 2021
"""

import numpy as np
import matplotlib.pyplot as plt


class UCB:
    def __init__(self, c=10, tmax=100):
        """ Initializes an instance of the Upper Confidence Bound
            (UCB) reinforcement learning algorithm.

            params
            float c:       controls level of exploration
            float tmax:    number of timesteps in which uncertainty of choices
                           are taken into account (exploitation when t > tmax)
            attrs
            float t:       timestep
            float decay:   decay rate of exploration constant c
            float rewards: stores the rewards received for each action

            returns: UCB object
        """
        self.__t = 0
        self.__tmax = tmax

        self.__c = c
        self.__decay = c / tmax
        self.__rewards = dict()

    @property
    def Q_table(self):
        """ Returns the Q-table with, for each action, its estimated value
            (i.e. average reward received).
        """
        return {a:np.mean(r) for a, r in self.__rewards.items() if len(r)}

    def __score__(self, action):
        """ Computes the UCB score for an action from its average reward (Q)
            and the uncertainty/doubt associated with the action (U).

            params
            str action: an action

            returns:    UCB score of the action
        """
        value = np.mean(self.__rewards[action])
        doubt = self.__c * np.sqrt(np.log(self.__t) / len(self.__rewards[action]))
        return value + doubt

    def select_action(self, actions):
        """ Selects an action from the set of available actions that maximizes
            the average observed reward, taking into account uncertainty.

            params
            list actions: List of actions from which to select

            returns: an action
        """
        self.__t += 1

        action_scores = []
        for action in actions:

            # Add new actions to reward table
            if action not in self.__rewards:
                self.__rewards[action] = []

            # Score action
            if self.__rewards[action] == []:
                score = np.inf # ensures all actions are sampled at least once
            else:
                score = self.__score__(action)
            action_scores.append((action, score))

        # Greedy selection
        selected_action, _ = max(action_scores, key=lambda x: x[1])
        return selected_action

    def update_utility(self, action, reward):
        """ Updates the action-value table (Q) by incrementally updating the average
            reward estimate of some action with the observed reward.

            params
            str action:    the selected action 
            float reward:  the reward obtained after performing the action

            returns: None
        """
        self.__rewards[action].append(reward)

        # Update exploration tendency
        self.__c = max(self.__c - self.__decay, 0)

    def plot(self, margin=0.05, line_width=0.005):
        """ Visualizes the Q scores and their uncertainties in a box plot.

            params
            float margin: Margin between bars

            returns: None
        """
        if len(self.__rewards) == 0:
            print("\tCannot plot empty reward table")
            return
        
        # Estimate value/uncertainty
        Q, U = [], []
        actions = sorted(self.__rewards.keys())
        for action in actions:

            if self.__rewards[action] == []:
                q = line_width
            else:
                q = np.mean(self.__rewards[action])

            u = self.__c * np.sqrt(np.log(self.__t) / (len(self.__rewards[action]) + 1))
            Q.append(q); U.append(u)

        # Draw barplots for U and Q
        fig = plt.figure(figsize=(10, 5), tight_layout=True)
        plt.suptitle("$t=${}, $c=${}".format(self.__t, round(self.__c, 3)))
        
        plt.subplot(1, 2, 1)
        plt.ylabel("$Uncertainty$ $(U)$")
        plt.xlabel("$Actions$ $(a)$")
        plt.xticks(range(len(actions)), actions)
        plt.bar(range(len(actions)), U)
        
        plt.subplot(1, 2, 2)
        plt.ylabel("$Utility$ $(Q)$")
        plt.xlabel("$Actions$ $(a)$")
        plt.xticks(range(len(actions)), actions)
        plt.bar(range(len(actions)), Q)
        plt.show()


"""
# Toy example with binomial rewards
if __name__ == "__main__":
    num_actions = 10
    actions = np.arange(num_actions)
    prob_reward = np.linspace(0.0, 1.0, num_actions)
    N = 100

    ucb = UCB()
    for k in range(N):
        # What actions are available?
        n = np.random.randint(3, num_actions)
        idx = np.random.choice(actions, size=n, replace=False)

        # What action to we pick?
        sampled_actions = actions[idx]
        action = ucb.select_action(sampled_actions)
        print(sampled_actions, action)

        # Draw reward from binomial distribution
        reward = np.random.binomial(1, prob_reward[action])

        # Update action reward
        ucb.update_utility(action, reward)

    ucb.plot()
"""
